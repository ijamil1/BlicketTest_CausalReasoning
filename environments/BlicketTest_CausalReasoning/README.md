# BlicketTest_CausalReasoning

### Overview
- **Environment ID**: `BlicketTest_CausalReasoning`
- **Short description**: Multi-turn causal reasoning environment based on the Blicket detector paradigm from developmental psychology. Tests an LLM's ability to design experiments, reason causally, and identify which objects are Blickets.
- **Tags**: multi-turn, reasoning, eval, train

### Reference
Based on [Do LLMs Think Like Scientists? Causal Reasoning and Hypothesis Testing in LLMs](https://arxiv.org/pdf/2505.09614).

### Task
- **Type**: multi-turn
- **Parser**: XMLParser (fields: `reasoning`, `action`)
- **Rubric overview**: Reward is a weighted combination of four active components: blicket set Jaccard similarity (0.5), per-step efficiency vs. the optimal agent (0.3), exploration efficiency (0.1), and format compliance (0.1). `hypotheses_eliminated` is retained at weight 0.0 for metric logging.

The agent interacts with a simulated "Blicket-detecting machine" across two phases:
1. **Exploration phase** — toggle objects on/off the machine one at a time, observe whether the machine activates, and exit when ready.
2. **Answer phase** — declare which objects are Blickets. The agent has up to `MAX_ANSWER_ATTEMPTS` (3) retries to produce a correctly-formatted answer before the episode ends with no score.

Each example's configuration — number of objects, which objects are Blickets (at least 2, up to floor(n/2)), and rule type — is fixed at dataset generation time. The hidden rule is either **disjunctive** (machine activates if *any* Blicket is present) or **conjunctive** (machine activates only if *all* Blickets are present). Neither the rule type nor the Blicket assignments are revealed to the agent.

### Quickstart
Run an evaluation with default settings:

```bash
prime eval run BlicketTest_CausalReasoning
```

Configure model and sampling:

```bash
prime eval run BlicketTest_CausalReasoning \
  -m openai/gpt-4.1-mini \
  -n 50 -r 3 -t 4096 -T 0.7 \
  -a '{"num_examples": 50}'
```

### Environment Arguments

| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `num_examples` | int | `250` | Number of training examples (clamped to [100, 500]) |

### Dataset Generation

Training and eval datasets are generated independently and are guaranteed to be fully disjoint.

**Training** (`num_examples` configs, n ∈ [4, 10], seed=42):
- Rule split: ~2/3 conjunctive, ~1/3 disjunctive. Conjunctive problems are oversampled because they are empirically harder for LLMs.
- Generated by calling `sample_balanced_configs` with `n_conjunctive = round(2 * num_examples / 3)` and `n_disj = num_examples - n_conj`.
- Sliced from a pre-generated max-size pool (333 conjunctive + 167 disjunctive), so the set of training configs is stable as `num_examples` changes — larger values just take more from the front of each rule-type list.

**Eval** (100 configs total, seed=100):
- 80 problems from n ∈ [4, 10]: 40 conjunctive + 40 disjunctive, guaranteed 50-50 split.
- 20 problems from n ∈ [11, 15]: 10 conjunctive + 10 disjunctive (disjoint from training by construction due to non-overlapping n range).
- All [4, 10] eval configs are generated with the full 500-config training pool excluded, so eval is disjoint from training for any valid `num_examples`.

### Architecture

`BlicketEnv` subclasses `vf.MultiTurnEnv`. The verifiers `max_turns` is set to `global_max_steps + 1 + MAX_ANSWER_ATTEMPTS` (the largest step budget across all rows + transition turn + up to 3 answer-phase retries).

**Rollout lifecycle:**

1. `setup_state()` reads per-row config from the dataset `info` field (blickets, rule type, step budget, and pre-computed optimal agent data). Initializes zeroed object states, the full hypothesis space (2^N blicket assignments × 2 rule types), and tracking counters.
2. `env_response()` drives the game loop across both phases. All turns increment `exploration_and_answer_count`:
   - **Exploration**: calls `parse_response(..., "exploration", ...)` which strips reasoning blocks, requires exactly one `<action>` tag, and delegates to `parse_action`. Validates the action, toggles object state, computes machine activation, filters the hypothesis space against the observation, records hypotheses eliminated this step into `hypotheses_eliminated_per_step`, and returns a compact observation. Invalid and redundant actions still consume a step.
   - **Answer**: calls `parse_response(..., "answer", ...)` which applies the same strict tag rules then delegates to `parse_blicket_set`. On successful parse, scores with Jaccard similarity and terminates. On failure, sends a reformat message and loops up to `MAX_ANSWER_ATTEMPTS` (3) total attempts; if all exhausted, exits with score 0.
3. Termination is handled by the base class `has_final_env_response` stop condition.

**Strict action parsing (`parse_response`):**

All action extraction goes through `parse_response(content, phase, num_objects)` which enforces:
- All `<reasoning>...</reasoning>` blocks are stripped before searching for `<action>` tags.
- Exactly one `<action>...</action>` must remain — zero or multiple tags yield `None` (unparseable).
- The extracted action string is then passed to the phase-specific parser (`parse_action` or `parse_blicket_set`).

**Machine activation logic:**
- **Disjunctive** (OR): machine ON if *any* Blicket is on the machine.
- **Conjunctive** (AND): machine ON only if *all* Blickets are on the machine.

**Transition to answer phase** happens when the agent sends `exit` or exhausts `max_num_steps`. The transition message includes a full observation history recap so the agent can reason over all experiments at once.

**Action format (exploration):**
```xml
<reasoning>...</reasoning>
<action>put 3 on</action>
```
Valid actions: `put {id} on|off` (1-indexed) or `exit`.

**Answer format:**
```xml
<reasoning>...</reasoning>
<action>{1, 3}</action>
```
List only the IDs of objects believed to be Blickets inside curly braces. Use `<action>{}</action>` if no objects are believed to be Blickets.

### File Structure (`BlicketTest_CausalReasoning.py`)

**Module-level constants:**
- `MAX_ANSWER_ATTEMPTS = 3` — maximum answer-phase retries before the episode ends with score 0.

**Entry point:**
- `load_environment(num_examples)` — generates training and eval datasets, builds the parser/rubric, and returns a `BlicketEnv` instance. See [Dataset Generation](#dataset-generation) above.

**Environment class:**
- `BlicketEnv(vf.MultiTurnEnv)`
  - `setup_state()` — reads pre-computed per-row config from dataset `info`. Initializes blicket array, zeroed object/machine states, step counter, phase tracker, history log, hypothesis space, and action-tracking counters (`total_action_count`, `exploration_and_answer_count`, `parseable_action_count`, `valid_action_count`, `redundant_action_count`, `out_of_range_count`, `answer_attempt_count`). Also loads `optimal_hypotheses_eliminated` and `optimal_hyp_eliminated_per_step` for use by reward functions.
  - `env_response()` — core game loop. Handles exploration (parse action via `parse_response`, validate, toggle, compute machine state, filter hypotheses, record per-step eliminations, return observation) and answer phase (parse predictions via `parse_response`, retry loop up to `MAX_ANSWER_ATTEMPTS`, score and signal termination on success).
  - `_build_transition_message()` — assembles the observation history recap when moving to answer phase.

**Helper functions:**
- `compute_machine_state()` — vectorized OR/AND activation over Blicket positions.
- `is_consistent()` — checks if a blicket assignment is consistent with an observed machine state.
- `parse_action()` — regex parser for `put {id} on|off` and `exit`.
- `parse_blicket_set()` — regex parser for `{1, 3}` answer format; returns a `set[int]` of predicted Blicket IDs, or `None` on failure.
- `parse_response()` — strict action extractor: strips reasoning blocks, requires exactly one `<action>` tag, then delegates to `parse_action` (exploration) or `parse_blicket_set` (answer). Returns `None` on any violation.
- `build_system_prompt()` — constructs the system prompt with rules, action format, and strategy hint (does not reveal the rule type).
- `build_initial_message()` — constructs the opening user message presenting the game.
- `format_observation()` — formats a compact observation (step counter, action, object lists, machine state).
- `format_history()` — formats the full observation history for the transition message.
- `compute_optimal_steps(num_objects, blickets, rule_type, num_samples, seed)` — simulates a greedy info-gain-maximizing agent `num_samples` times. Returns `(avg_steps, total_hypotheses_to_eliminate, per_step_avg_eliminated)`, where `per_step_avg_eliminated[t]` is the average hypotheses eliminated at step `t+1` across all simulation trajectories that were still active at that step.
- `sample_unique_configs(num_objects_range, n, seed)` — rejection-samples `n` unique `(n_obj, rule, blickets)` configs from the given n range with no rule-balance constraint.
- `sample_balanced_configs(num_objects_range, n_conjunctive, n_disjunctive, exclude_keys, seed)` — rejection-samples exactly `n_conjunctive` conjunctive and `n_disjunctive` disjunctive configs, skipping any keys in `exclude_keys` to guarantee disjointness with another set.
- `build_rows(configs)` — converts a list of configs into HuggingFace dataset rows. For each config, calls `compute_optimal_steps` (with a deterministic per-config seed derived from MD5) and stores `optimal_hypotheses_eliminated` and `optimal_hyp_eliminated_per_step` in the `info` JSON field.

**Reward functions:**
- `blicket_set_jaccard()` — reads `state["final_score"]`, set by `env_response` as the Jaccard similarity between the predicted Blicket set and the gold set. Returns 0.0 if no valid answer was recorded.
- `exploration_efficiency()` — `1 - (wasted / parseable_action_count)`, where waste = redundant actions + out-of-range object IDs + non-contiguous configuration revisits. Higher is better.
- `format_compliance()` — `parseable_action_count / exploration_and_answer_count` across all turns in both phases. Higher is better.
- `hypotheses_eliminated()` — fraction of total hypotheses eliminated relative to the theoretical maximum (`2^(N+1) - 1`). Weight 0.0 (retained for metric logging only).
- `per_step_efficiency()` — iterates over the optimal agent's steps. For each step `t` where `optimal_avg[t] > 0`: if the agent took that step, `ratio = min(1.0, agent_elim[t] / optimal_avg[t])`; if the agent exited before step `t`, `ratio = 0.0` (penalty for under-exploration). Returns the mean of all included ratios. Steps where `optimal_avg[t] == 0` are excluded from the average.

### Counters

| Counter | Incremented when |
|---|---|
| `exploration_and_answer_count` | Every `env_response` call (both phases) |
| `total_action_count` | Every exploration-phase `env_response` call |
| `parseable_action_count` | Exploration: `parse_response` returns non-None. Answer: valid predictions submitted |
| `valid_action_count` | Exit action or non-redundant in-range toggle |
| `redundant_action_count` | Toggle targeting object already in requested state |
| `out_of_range_count` | Toggle with object ID outside [1, num_objects] |
| `answer_attempt_count` | Each answer-phase `env_response` call |

### Reward / Metrics

| Component | Weight | Meaning |
| --------- | ------ | ------- |
| `blicket_set_jaccard` | 0.5 | Jaccard similarity between predicted and gold Blicket sets |
| `per_step_efficiency` | 0.3 | Average ratio of agent's hypotheses eliminated per step vs. the optimal greedy agent; penalizes under-exploration |
| `exploration_efficiency` | 0.1 | `1 - (wasted / parseable)` — fraction of productive actions |
| `format_compliance` | 0.1 | Parseable actions across all turns (both phases) |
| `hypotheses_eliminated` | 0.0 | Fraction of hypotheses eliminated vs. theoretical maximum (metric only) |
